<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><style type=text/css>body{font-family:monospace}</style><title>The parallelism between testing and monitoring strategies</title><link rel=stylesheet href=/css/style.css><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script></head><body><header>=============<br>== <a href=https://stefanoboriero.github.io/>Stefano</a> ==<br>=============<div style=float:right>A personal blog on any topic, with a strong focus on Observability and Software Engineering</div><br><p><nav><a href=/><b>Start</b></a>.
<a href=/posts/><b>Posts</b></a>.
<a href=/posts/about><b>About</b></a>.
<a href=/tags/><b>Tags</b></a>.</nav></p></header><main><article><h1>The parallelism between testing and monitoring strategies</h1><b><time>19.08.2024</time></b>
<a href=/tags/observability>observability</a><div><p>At the beginning of my career, I was tasked with testing a ticket developed by a very senior developer on my team. When I opened the test instructions, to my surprise, I found this:</p><blockquote><p>Just deploy the changes to the staging environment. If something is not working, we&rsquo;ll get alerted.</p></blockquote><p>It was only a few years later when I really understood the wisdom behind this sentence, and this article will try to explain why alerting should be tought as a superset of testing.</p><blockquote><p>Good <em>testing</em> aims at ensuring that <em>the system is behaving as expected at rest</em>.</p></blockquote><blockquote><p>Good <em>monitoring</em> aims at ensuring that <em>the system is behaving as expected at runtime</em>.</p></blockquote><p>The parallelism doesn&rsquo;t end here. Testing has been long discussed in Software Engineering and there are certain properties of sound testing suites that can be extended to provide a blueprint for implementing sound monitoring strategies.</p><blockquote><p>In testing, we want to avoid flaky test. In monitoring, we want to minimise false positives alerts.</p></blockquote><p>Flaky tests are tests that do not always succeed, with seemingly random and inexplicable failures. When test failures rate increases, engineers start ignoring those test as they are seen as not reliable. This is problematic because often the reason for the flakiness is an actual bug. The parallelism with alerting here is with false positive alerts. A false positive alert is an alert that fires when sometimes there isn&rsquo;t a real problem: this has the same effect as flaky test, alerts with a high rate of false positives are destined to be ignored in the long run, even if they might be signalling a problem!</p><blockquote><p>In testing, we want to test the behaviour of the system, not its implementation. In monitoring, we want to alert on misbehaviours, not on its causes.</p></blockquote><p>One of the mantras of testing is that you shouldn&rsquo;t be testing implementation details of your system, but instead that the system is doing what it&rsquo;s behaving and keeping its invariants. At the same time, we should alert on problems not causes. The reason is that when the cause manifests, it is not ensured that it will lead to an actual problem. Take a web service as an example, which expected behaviour is to serve traffic in a timely manner, that sometimes is subject to long GC pauses: if such pauses happen when the system is serving a high volume of traffic, it&rsquo;s likely that the system won&rsquo;t serve all of it in a timely manner, and you should be alerted. However if such pauses happen when no traffic is being served, you shouldn&rsquo;t be alerted. SLO practice help in this by providing a framework for defining acceptable levels of service for your systems and building alerts based on them.</p><blockquote><p>In testing, we want to write testeable code. In monitoring, we want to write observable code.</p></blockquote><p>Sometimes is hard to write good quality testing on a codebase that is not lending itself to it. For this reason, practices like TDD are preached forcing engineers to structure their code such that testing is easy and sound. In the same way, code should be written such that is easy to instrument and measure the behaviour of the system.</p><p>In a nutshell, a good monitoring infrastructure should be in fact a continous test suite running on your system.</p></div></article></main><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=/posts/observability/>Making software systems observable</a></li><li><a href=/posts/slo/>SLO practice with SpringBoot and Prometheus</a></li><li><a href=/posts/alerting-as-testing/>The parallelism between testing and monitoring strategies</a></li><li><a href=/posts/a-day-in-kubernetes/>A day in Kubernetes</a></li><li><a href=/posts/intro/>Why does this webiste exist?</a></li></ul></div></div></aside><footer><p>&copy; 2025 <a href=https://stefanoboriero.github.io/><b>Stefano</b></a>.
<a href=https://github.com/StefanoBoriero><b>Github</b></a>.
<a href=https://www.linkedin.com/in/stefanoboriero/><b>LinkedIn</b></a>.
<a href=mailto:stefano.boriero@gmail.com><b>E-mail</b></a>.</p></footer></body></html>